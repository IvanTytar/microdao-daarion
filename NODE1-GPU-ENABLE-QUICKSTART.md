# âš¡ Ð¨Ð²Ð¸Ð´ÐºÐ¸Ð¹ ÑÑ‚Ð°Ñ€Ñ‚: Ð£Ð²Ñ–Ð¼ÐºÐ½ÐµÐ½Ð½Ñ GPU Ð´Ð»Ñ Ollama Ð½Ð° ÐÐžÐ”Ð1

**Ð”Ð°Ñ‚Ð°:** 2025-01-27  
**GPU:** âœ… NVIDIA RTX 4000 SFF Ada (20GB VRAM) - **Ð”ÐžÐ¡Ð¢Ð£ÐŸÐÐ˜Ð™!**  
**ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°:** Ollama Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ” CPU (1583% CPU) Ð·Ð°Ð¼Ñ–ÑÑ‚ÑŒ GPU  
**Ð Ñ–ÑˆÐµÐ½Ð½Ñ:** Ð£Ð²Ñ–Ð¼ÐºÐ½ÑƒÑ‚Ð¸ GPU acceleration

---

## âœ… ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ°

**GPU ÑÑ‚Ð°Ñ‚ÑƒÑ:**
```bash
nvidia-smi
# âœ… NVIDIA RTX 4000 SFF Ada Generation
# âœ… 20GB VRAM
# âœ… CUDA 12.2
# âœ… Driver 535.274.02
```

**Docker GPU Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼ÐºÐ°:**
```bash
nvidia-container-cli --version
# âœ… 1.18.0

docker --version
# âœ… 29.0.1 (Ð¿Ñ–Ð´Ñ‚Ñ€Ð¸Ð¼ÑƒÑ” --gpus)
```

**ÐŸÐ¾Ñ‚Ð¾Ñ‡Ð½Ðµ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ GPU:**
- Python Ð¿Ñ€Ð¾Ñ†ÐµÑ: 2240 MiB VRAM (11%)
- GPU utilization: 0%
- **Ollama ÐÐ• Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ” GPU!**

---

## ðŸš€ Ð¨Ð²Ð¸Ð´ÐºÐµ Ñ€Ñ–ÑˆÐµÐ½Ð½Ñ

### âœ… Ð’Ð°Ñ€Ñ–Ð°Ð½Ñ‚ 1: Ollama ÑÐº systemd service (ÐŸÐžÐ¢ÐžÐ§ÐÐ ÐšÐžÐÐ¤Ð†Ð“Ð£Ð ÐÐ¦Ð†Ð¯)

**Ð—Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾:** Ollama Ð¿Ñ€Ð°Ñ†ÑŽÑ” ÑÐº systemd service (`/etc/systemd/system/ollama.service`)

**Ð¨Ð²Ð¸Ð´ÐºÐµ Ñ€Ñ–ÑˆÐµÐ½Ð½Ñ (Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡Ð½Ð¸Ð¹ ÑÐºÑ€Ð¸Ð¿Ñ‚):**
```bash
# ÐÐ° ÐÐžÐ”Ð1
cd /opt/microdao-daarion
# Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ð¸ ÑÐºÑ€Ð¸Ð¿Ñ‚
bash NODE1-OLLAMA-GPU-ENABLE.sh
```

**ÐÐ±Ð¾ Ð²Ñ€ÑƒÑ‡Ð½Ñƒ:**
```bash
# 1. Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸Ñ‚Ð¸ override.conf
sudo mkdir -p /etc/systemd/system/ollama.service.d
sudo tee /etc/systemd/system/ollama.service.d/override.conf > /dev/null << 'EOF'
[Service]
Environment="OLLAMA_NUM_GPU=1"
Environment="OLLAMA_GPU_LAYERS=35"
Environment="CUDA_VISIBLE_DEVICES=0"
Environment="OLLAMA_KEEP_ALIVE=24h"
EOF

# 2. ÐŸÐµÑ€ÐµÐ·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶Ð¸Ñ‚Ð¸ systemd
sudo systemctl daemon-reload

# 3. ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ð¸ Ollama
sudo systemctl restart ollama

# 4. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€Ð¸Ñ‚Ð¸
nvidia-smi
curl http://localhost:11434/api/ps
```

### Ð’Ð°Ñ€Ñ–Ð°Ð½Ñ‚ 2: Ollama ÑÐº Docker ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€

Ð¯ÐºÑ‰Ð¾ Ollama Ð¿Ñ€Ð°Ñ†ÑŽÑ” Ð² Docker (Ð°Ð»Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€ Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾, Ð¼Ð¾Ð¶Ð»Ð¸Ð²Ð¾ Ð² Ñ–Ð½ÑˆÐ¾Ð¼Ñƒ compose Ñ„Ð°Ð¹Ð»Ñ–):

```bash
# Ð—Ð½Ð°Ð¹Ñ‚Ð¸ Ollama
docker ps -a | grep ollama
docker compose -f /opt/microdao-daarion/docker-compose.yml ps | grep ollama

# ÐžÐ½Ð¾Ð²Ð¸Ñ‚Ð¸ docker-compose.yml
cd /opt/microdao-daarion
nano docker-compose.yml

# Ð”Ð¾Ð´Ð°Ñ‚Ð¸ GPU ÐºÐ¾Ð½Ñ„Ñ–Ð³ÑƒÑ€Ð°Ñ†Ñ–ÑŽ Ð´Ð»Ñ ollama service:
services:
  ollama:
    image: ollama/ollama:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_NUM_GPU=1
      - OLLAMA_GPU_LAYERS=35

# ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ð¸
docker compose up -d ollama
```

### Ð’Ð°Ñ€Ñ–Ð°Ð½Ñ‚ 3: Ollama Ð¿Ñ€Ð°Ñ†ÑŽÑ” Ð½Ð° Ñ…Ð¾ÑÑ‚Ñ– (Ð½Ðµ Ð² Docker)

Ð¯ÐºÑ‰Ð¾ Ollama Ð²ÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð½Ð° Ñ…Ð¾ÑÑ‚Ñ–:

```bash
# ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€Ð¸Ñ‚Ð¸ Ollama
ollama --version
which ollama

# Ollama Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡Ð½Ð¾ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ” GPU ÑÐºÑ‰Ð¾ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹ CUDA
# ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€Ð¸Ñ‚Ð¸:
ollama ps
nvidia-smi

# Ð¯ÐºÑ‰Ð¾ Ð½Ðµ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ” GPU, Ð¿ÐµÑ€ÐµÐ²Ñ–Ñ€Ð¸Ñ‚Ð¸ CUDA:
nvcc --version
```

---

## ðŸ“Š ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð²

```bash
# 1. GPU utilization (Ð¼Ð°Ñ” Ð·Ð±Ñ–Ð»ÑŒÑˆÐ¸Ñ‚Ð¸ÑÑ)
nvidia-smi

# 2. CPU Ð½Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ollama (Ð¼Ð°Ñ” Ð·Ð½Ð¸Ð·Ð¸Ñ‚Ð¸ÑÑ)
top -bn1 | grep ollama

# 3. Ollama Ñ‡ÐµÑ€ÐµÐ· API
curl http://localhost:11434/api/ps

# 4. Ð¢ÐµÑÑ‚ ÑˆÐ²Ð¸Ð´ÐºÐ¾ÑÑ‚Ñ–
time curl -X POST http://localhost:11434/api/generate -d '{
  "model": "qwen3:8b",
  "prompt": "ÐŸÑ€Ð¸Ð²Ñ–Ñ‚, Ñ‚ÐµÑÑ‚ GPU",
  "stream": false
}'
```

**ÐžÑ‡Ñ–ÐºÑƒÐ²Ð°Ð½Ñ– Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸:**
- âœ… GPU utilization: 0% â†’ 30-50%
- âœ… CPU Ollama: 1583% â†’ 50-100%
- âœ… Ð—Ð°Ð³Ð°Ð»ÑŒÐ½Ðµ CPU: 85.3% â†’ 40-50%
- âœ… Ð¨Ð²Ð¸Ð´ÐºÑ–ÑÑ‚ÑŒ: +200-300%

---

## ðŸ” Ð”Ñ–Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ°

Ð¯ÐºÑ‰Ð¾ GPU Ð½Ðµ Ð¿Ñ€Ð°Ñ†ÑŽÑ”:

```bash
# 1. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€Ð¸Ñ‚Ð¸ CUDA Ð² Ollama
docker exec ollama nvidia-smi  # Ð°Ð±Ð¾ Ð½Ð° Ñ…Ð¾ÑÑ‚Ñ–
ollama ps  # Ð¿Ð¾ÐºÐ°Ð¶Ðµ Ñ‡Ð¸ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ” GPU

# 2. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€Ð¸Ñ‚Ð¸ Ð»Ð¾Ð³Ð¸
docker logs ollama | grep -i gpu
journalctl -u ollama | grep -i gpu

# 3. ÐŸÐµÑ€ÐµÐ²Ñ–Ñ€Ð¸Ñ‚Ð¸ CUDA Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ–ÑÑ‚ÑŒ
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
```

---

**Last Updated:** 2025-01-27  
**Status:** âœ… GPU Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹, Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ð¾ Ð½Ð°Ð»Ð°ÑˆÑ‚ÑƒÐ²Ð°Ñ‚Ð¸ Ollama

