# ğŸš€ PHASE 3 READY â€” LLM Proxy + Memory + Tools

**Status:** ğŸ“‹ Ready to implement  
**Dependencies:** Phase 2 complete âœ…  
**Estimated Time:** 6-8 weeks  
**Priority:** High

---

## ğŸ¯ Goal

Ğ—Ñ€Ğ¾Ğ±Ğ¸Ñ‚Ğ¸ Ğ°Ğ³ĞµĞ½Ñ‚Ñ–Ğ² DAARION Ğ¿Ğ¾-ÑĞ¿Ñ€Ğ°Ğ²Ğ¶Ğ½ÑŒĞ¾Ğ¼Ñƒ Ñ€Ğ¾Ğ·ÑƒĞ¼Ğ½Ğ¸Ğ¼Ğ¸:
- **LLM Proxy** â€” Ñ”Ğ´Ğ¸Ğ½Ğ° Ñ‚Ğ¾Ñ‡ĞºĞ° Ğ´Ğ»Ñ Ğ²ÑÑ–Ñ… LLM Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ñ–Ğ² (OpenAI, DeepSeek, Local)
- **Memory Orchestrator** â€” Ñ”Ğ´Ğ¸Ğ½Ğ¸Ğ¹ API Ğ´Ğ»Ñ short/mid/long-term Ğ¿Ğ°Ğ¼Ê¼ÑÑ‚Ñ–
- **Toolcore** â€” Ñ€ĞµÑ”ÑÑ‚Ñ€ Ñ–Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ–Ğ² + Ğ±ĞµĞ·Ğ¿ĞµÑ‡Ğ½Ğµ Ğ²Ğ¸ĞºĞ¾Ğ½Ğ°Ğ½Ğ½Ñ

**Phase 3 = Infrastructure for Agent Intelligence**

---

## ğŸ“¦ What Will Be Built

### 1. LLM Proxy Service
**Port:** 7007  
**Purpose:** Unified LLM gateway

**Features:**
- âœ… Multi-provider support (OpenAI, DeepSeek, Local)
- âœ… Model routing (logical â†’ physical models)
- âœ… Usage logging (tokens, latency per agent)
- âœ… Rate limiting per agent
- âœ… Cost tracking hooks

**API:**
```http
POST /internal/llm/proxy
{
  "model": "gpt-4.1-mini",
  "messages": [...],
  "metadata": { "agent_id": "...", "microdao_id": "..." }
}
```

**Deliverables:** 10 files
- `main.py`, `models.py`, `router.py`
- `providers/` (OpenAI, DeepSeek, Local)
- `config.yaml`, `Dockerfile`, `README.md`

---

### 2. Memory Orchestrator Service
**Port:** 7008  
**Purpose:** Unified memory API

**Features:**
- âœ… Short-term memory (channel context)
- âœ… Mid-term memory (agent RAG)
- âœ… Long-term memory (knowledge base)
- âœ… Vector search (embeddings)
- âœ… Memory indexing pipeline

**API:**
```http
POST /internal/agent-memory/query
{
  "agent_id": "agent:sofia",
  "microdao_id": "microdao:7",
  "query": "What were recent changes?",
  "limit": 5
}

POST /internal/agent-memory/store
{
  "agent_id": "...",
  "content": { "user_message": "...", "agent_reply": "..." }
}
```

**Deliverables:** 9 files
- `main.py`, `models.py`, `router.py`
- `backends/` (PostgreSQL, Vector Store, KB)
- `embedding_client.py`, `config.yaml`, `README.md`

---

### 3. Toolcore Service
**Port:** 7009  
**Purpose:** Tool registry + execution

**Features:**
- âœ… Tool registry (config-based â†’ DB-backed later)
- âœ… Permission checks (agent â†’ tool mapping)
- âœ… HTTP executor (call external services)
- âœ… Python executor (optional, for internal functions)
- âœ… Error handling + timeouts

**API:**
```http
GET /internal/tools
â†’ List available tools

POST /internal/tools/call
{
  "tool_id": "projects.list",
  "agent_id": "agent:sofia",
  "args": { "microdao_id": "microdao:7" }
}
```

**Deliverables:** 8 files
- `main.py`, `models.py`, `registry.py`
- `executors/` (HTTP, Python)
- `config.yaml`, `Dockerfile`, `README.md`

---

## ğŸ”„ Updated Architecture

### Before (Phase 2):
```
agent-runtime:
  - Mock LLM responses
  - Optional memory
  - No tools
```

### After (Phase 3):
```
agent-runtime:
  â†“
  â”œâ”€ LLM Proxy â†’ [OpenAI | DeepSeek | Local]
  â”œâ”€ Memory Orchestrator â†’ [Vector DB | PostgreSQL]
  â””â”€ Toolcore â†’ [projects.list | task.create | ...]
```

---

## ğŸ¯ Acceptance Criteria

### LLM Proxy:
- âœ… 2+ providers working (e.g., OpenAI + Local stub)
- âœ… Model routing from config
- âœ… Usage logging per agent
- âœ… Health checks pass

### Memory Orchestrator:
- âœ… Query returns relevant memories
- âœ… Store saves new memories
- âœ… Vector search works (simple cosine)
- âœ… agent-runtime integration

### Toolcore:
- âœ… Tool registry loaded from config
- âœ… 1+ tool working (e.g., projects.list)
- âœ… Permission checks work
- âœ… HTTP executor functional

### E2E:
- âœ… Agent uses real LLM (not mock)
- âœ… Agent uses memory (RAG)
- âœ… Agent can call tools
- âœ… Full flow: User â†’ Agent (with tool) â†’ Reply

---

## ğŸ“… Timeline

| Week | Focus | Deliverables |
|------|-------|--------------|
| 1-2 | LLM Proxy | Service + 2 providers |
| 3-4 | Memory Orchestrator | Service + vector search |
| 5-6 | Toolcore | Service + 1 tool |
| 7 | Integration | Update agent-runtime |
| 8 | Testing | E2E + optimization |

**Total:** 8 weeks (6-8 weeks realistic)

---

## ğŸš€ How to Start

### Option 1: Cursor AI

```bash
# Copy Phase 3 master task
cat docs/tasks/PHASE3_MASTER_TASK.md | pbcopy

# Paste into Cursor AI
# Wait for implementation (~1-2 hours per service)
```

### Option 2: Manual

```bash
# 1. Start with LLM Proxy
mkdir -p services/llm-proxy
cd services/llm-proxy
# Follow PHASE3_MASTER_TASK.md

# 2. Then Memory Orchestrator
mkdir -p services/memory-orchestrator
# ...

# 3. Then Toolcore
mkdir -p services/toolcore
# ...
```

---

## ğŸ”— Key Files

### Specification:
- [PHASE3_MASTER_TASK.md](docs/tasks/PHASE3_MASTER_TASK.md) â­ **Main task**
- [PHASE3_ROADMAP.md](docs/tasks/PHASE3_ROADMAP.md) â€” Detailed planning

### Phase 2 (Complete):
- [PHASE2_COMPLETE.md](PHASE2_COMPLETE.md) â€” What's already built
- [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md)

---

## ğŸ’¡ Key Concepts

### LLM Proxy:
- **Logical models** (gpt-4.1-mini) â†’ **Physical providers** (OpenAI API)
- Routing via config
- Cost tracking per agent
- Graceful fallbacks

### Memory Orchestrator:
- **Short-term:** Recent channel messages
- **Mid-term:** RAG embeddings (conversations, tasks)
- **Long-term:** Knowledge base (docs, roadmaps)
- Vector search for relevance

### Toolcore:
- **Static registry** (config.yaml) â†’ **Dynamic registry** (DB) later
- **HTTP executor:** Call external services
- **Permission model:** Agent â†’ Tool allowlist
- **Error handling:** Timeouts, retries

---

## ğŸ“Š Service Ports

| Service | Port | Purpose |
|---------|------|---------|
| messaging-service | 7004 | REST + WebSocket |
| agent-filter | 7005 | Filtering |
| agent-runtime | 7006 | Agent execution |
| **llm-proxy** | **7007** | **LLM gateway** âœ¨ |
| **memory-orchestrator** | **7008** | **Memory API** âœ¨ |
| **toolcore** | **7009** | **Tool execution** âœ¨ |
| router | 8000 | Event routing |

---

## ğŸ“ What You'll Learn

### Technologies:
- LLM API integration (OpenAI, DeepSeek)
- Vector embeddings + similarity search
- Tool execution patterns
- Provider abstraction
- Cost tracking
- Rate limiting

### Architecture:
- Gateway pattern (LLM Proxy)
- Orchestrator pattern (Memory)
- Registry pattern (Toolcore)
- Multi-provider routing
- Graceful degradation

---

## ğŸ› Expected Challenges

### LLM Proxy:
- API key management
- Rate limits from providers
- Cost control
- Streaming support (Phase 3.5)

**Mitigation:**
- Environment variables for keys
- In-memory rate limiting
- Usage logging
- Streaming as TODO

### Memory Orchestrator:
- Vector search performance
- Embedding generation latency
- Memory indexing pipeline
- Relevance tuning

**Mitigation:**
- Simple cosine similarity first
- Async embedding generation
- Background indexing jobs
- A/B testing for relevance

### Toolcore:
- Tool permission model
- Execution sandboxing
- Error handling
- Tool discovery

**Mitigation:**
- Config-based permissions v1
- HTTP executor with timeouts
- Comprehensive error types
- Static registry â†’ DB later

---

## ğŸ”œ After Phase 3

### Phase 3.5 (Optional Enhancements):
- Streaming LLM responses
- Advanced memory strategies
- Tool composition
- Agent-to-agent communication

### Phase 4 (Next Major):
- Usage & Billing system
- Security (PDP/PEP)
- Advanced monitoring
- Agent marketplace

---

## âœ… Checklist Before Starting

### Prerequisites:
- âœ… Phase 2 complete and tested
- âœ… NATS running
- âœ… PostgreSQL running
- âœ… Docker Compose working
- âœ… OpenAI API key (optional, can use local)

### Recommended:
- Local LLM setup (Ollama/vLLM) for testing
- Vector DB exploration (pgvector extension)
- Review existing tools in your stack

---

## ğŸ‰ Success Looks Like

**After Phase 3:**
- âœ… Agent Sofia uses real GPT-4 (not mock)
- âœ… Agent remembers past conversations (RAG)
- âœ… Agent can list projects (tool execution)
- âœ… All flows < 5s latency
- âœ… Usage tracked per agent
- âœ… Production ready

**Example Flow:**
```
User: "Sofia, Ñ‰Ğ¾ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ² Ğ¿Ñ€Ğ¾Ñ”ĞºÑ‚Ñ– X?"
    â†“
agent-runtime:
  1. Query memory (past discussions about project X)
  2. Call tool: projects.list(microdao_id)
  3. Build prompt with context + tool results
  4. Call LLM Proxy (GPT-4)
  5. Post reply
    â†“
Sofia: "Ğ’ Ğ¿Ñ€Ğ¾Ñ”ĞºÑ‚Ñ– X Ñ” 3 Ğ½Ğ¾Ğ²Ñ– Ğ·Ğ°Ğ´Ğ°Ñ‡Ñ–:
1. Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ñ‚Ğ¸ Phase 2 Ñ‚ĞµÑÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ
2. ĞŸĞ¾Ñ‡Ğ°Ñ‚Ğ¸ Phase 3 LLM integration
3. ĞĞ½Ğ¾Ğ²Ğ¸Ñ‚Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ñ–Ñ
ĞÑÑ‚Ğ°Ğ½Ğ½Ñ” Ğ¾Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ Ğ±ÑƒĞ»Ğ¾ Ğ²Ñ‡Ğ¾Ñ€Ğ°."
```

---

## ğŸ“ Next Actions

### This Week:
1. âœ… Review PHASE3_MASTER_TASK.md
2. âœ… Decide: Cursor AI or manual
3. âœ… Set up OpenAI API key (or local LLM)
4. âœ… Review tool requirements

### Next Week:
1. ğŸ”œ Start LLM Proxy implementation
2. ğŸ”œ Test with 2 providers
3. ğŸ”œ Integrate with agent-runtime

---

**Status:** ğŸ“‹ ALL SPECS READY  
**Version:** 1.0.0  
**Last Updated:** 2025-11-24

**READY TO BUILD PHASE 3!** ğŸš€





