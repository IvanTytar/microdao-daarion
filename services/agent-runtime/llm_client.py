import httpx
import os

LLM_PROXY_URL = os.getenv("LLM_PROXY_URL", "http://llm-proxy:7007")

async def generate_response(model: str, messages: list[dict], max_tokens: int = 1000) -> str:
    """
    Call LLM Proxy to generate response
    
    Falls back to mock response if LLM Proxy is not available
    """
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{LLM_PROXY_URL}/internal/llm/proxy",
                headers={
                    "X-Internal-Secret": os.getenv("LLM_PROXY_SECRET", "dev-secret-token"),
                    "Content-Type": "application/json"
                },
                json={
                    "model": model,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "metadata": {
                        "agent_id": "agent:runtime",
                        "microdao_id": "microdao:daarion"
                    }
                }
            )
            response.raise_for_status()
            data = response.json()
            return data.get("content", "")
    except httpx.HTTPStatusError as e:
        print(f"‚ö†Ô∏è LLM Proxy HTTP error: {e.response.status_code}")
        return await generate_mock_response(messages)
    except httpx.ConnectError:
        print(f"‚ö†Ô∏è LLM Proxy not available, using mock response")
        return await generate_mock_response(messages)
    except Exception as e:
        print(f"‚ö†Ô∏è LLM error: {e}")
        return await generate_mock_response(messages)

async def generate_mock_response(messages: list[dict]) -> str:
    """
    Generate mock response based on user message
    
    This is used when LLM Proxy is not available (Phase 2 testing)
    """
    # Extract last user message
    user_message = ""
    for msg in reversed(messages):
        if msg.get("role") == "user":
            user_message = msg.get("content", "").lower()
            break
    
    # Simple keyword-based responses
    if "–ø—Ä–∏–≤—ñ—Ç" in user_message or "hello" in user_message or "hi" in user_message:
        return "–ü—Ä–∏–≤—ñ—Ç! –Ø Sofia, –∞—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ–º–∞–Ω–¥–∏ DAARION. –Ø–∫ –º–æ–∂—É –¥–æ–ø–æ–º–æ–≥—Ç–∏?"
    
    if "–¥–æ–ø–æ–º–æ–∂" in user_message or "help" in user_message:
        return "–ó–≤–∏—á–∞–π–Ω–æ! –Ø –º–æ–∂—É –¥–æ–ø–æ–º–æ–≥—Ç–∏ –∑:\n- –ü–ª–∞–Ω—É–≤–∞–Ω–Ω—è–º –∑–∞–¥–∞—á\n- –ü—ñ–¥—Å—É–º—É–≤–∞–Ω–Ω—è–º –æ–±–≥–æ–≤–æ—Ä–µ–Ω—å\n- –û—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ—î—é –ø—Ä–æ—î–∫—Ç—ñ–≤\n- –í—ñ–¥–ø–æ–≤—ñ–¥—è–º–∏ –Ω–∞ –ø–∏—Ç–∞–Ω–Ω—è –ø—Ä–æ —Å–∏—Å—Ç–µ–º—É\n\n–ü—Ä–æ —â–æ —Ö–æ—á–µ—Ç–µ –ø–æ–≥–æ–≤–æ—Ä–∏—Ç–∏?"
    
    if "–¥—è–∫—É" in user_message or "thank" in user_message:
        return "–ó–∞–≤–∂–¥–∏ —Ä–∞–¥–∏–π –¥–æ–ø–æ–º–æ–≥—Ç–∏! üòä"
    
    if "phase 2" in user_message or "—Ñ–∞–∑–∞ 2" in user_message:
        return "Phase 2 ‚Äî —Ü–µ —ñ–Ω—Ç–µ–≥—Ä–∞—Ü—ñ—è –∞–≥–µ–Ω—Ç—ñ–≤ —É Messenger! –Ø –≤–∂–µ –ø—Ä–∞—Ü—é—é —á–µ—Ä–µ–∑ agent-runtime, agent-filter —Ç–∞ DAGI Router. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –º–µ–Ω—ñ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ –Ω–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ –∫–∞–Ω–∞–ª–∞—Ö. üöÄ"
    
    if "?" in user_message:
        return "–¶–µ —Ü—ñ–∫–∞–≤–µ –ø–∏—Ç–∞–Ω–Ω—è! –ù–∞ –¥–∞–Ω–∏–π –º–æ–º–µ–Ω—Ç —è –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é mock LLM (Phase 2 testing mode). –ö–æ–ª–∏ –±—É–¥–µ –ø—ñ–¥–∫–ª—é—á–µ–Ω–æ —Å–ø—Ä–∞–≤–∂–Ω—ñ–π LLM Proxy, —è –∑–º–æ–∂—É –¥–∞–≤–∞—Ç–∏ –±—ñ–ª—å—à —Ä–æ–∑–≥–æ—Ä–Ω—É—Ç—ñ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ."
    
    # Default response
    return "–î—è–∫—É—é –∑–∞ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è! –Ø Sofia, —ñ —è —Ç—É—Ç —â–æ–± –¥–æ–ø–æ–º–æ–≥—Ç–∏. –ó–∞—Ä–∞–∑ –ø—Ä–∞—Ü—é—é –≤ —Ç–µ—Å—Ç–æ–≤–æ–º—É —Ä–µ–∂–∏–º—ñ (Phase 2), –∞–ª–µ —Å–∫–æ—Ä–æ –±—É–¥—É –ø—ñ–¥–∫–ª—é—á–µ–Ω–∞ –¥–æ –ø–æ–≤–Ω–æ—Ü—ñ–Ω–Ω–æ–≥–æ LLM. –©–æ –≤–∞—Å —Ü—ñ–∫–∞–≤–∏—Ç—å?"

